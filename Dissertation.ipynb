{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.testing import assert_almost_equal\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.optimize import minimize, brentq, newton\n",
    "\n",
    "from scipy.stats import norm, jarque_bera\n",
    "from statsmodels.stats.diagnostic import lilliefors\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90244378, 0.12724075, 0.27454395],\n",
       "       [0.77238418, 0.26468449, 0.99958099],\n",
       "       [0.11006019, 0.5899027 , 0.34662857]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(size = [3, 3])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Covariance\n",
    "\n",
    "def covariance_calc(H, k):\n",
    "    #Calculation derived in dissertation.\n",
    "    \n",
    "    return 0.5\\\n",
    "        * (np.abs(k - 1) ** (2 * H)\\\n",
    "         + np.abs(k + 1) ** (2 * H)\\\n",
    "         - 2 * (np.abs(k) ** (2 * H)))\n",
    "\n",
    "#Test basic functionality. Numbers calculated by hand\n",
    "assert_almost_equal(covariance_calc(0.5, 1), 0)\n",
    "assert_almost_equal(covariance_calc(0.25, 1), -0.2928932)\n",
    "assert_almost_equal(covariance_calc(0.25, -1), -0.2928932)\n",
    "assert_almost_equal(covariance_calc(0.75, 4), 0.1882461)\n",
    "\n",
    "def get_covariance_matrix(n_steps, H):\n",
    "    #Return an unscaled covariance matrix\n",
    "    m = np.zeros([n_steps , n_steps])\n",
    "    k = np.arange(0, n_steps, 1)\n",
    "\n",
    "    #Calculate one column, then use this to populate rest of lower triangle of matrix\n",
    "    m[:, 0] = covariance_calc(H, k)\n",
    "\n",
    "    for i in range(1, n_steps):\n",
    "        m[i:, i] = m[i-1:-1, i-1]\n",
    "    \n",
    "    m = np.tril(m) + np.triu(m.T, 1)\n",
    "    return m\n",
    "\n",
    "#Test matrix build right. Numbers calculated by hand. \n",
    "assert_almost_equal(get_covariance_matrix(3, 0.25), np.array([[1, -0.2928932, -0.0481881], [-0.2928932, 1, -0.2928932], [-0.0481881, -0.2928932, 1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Manipulation\n",
    "\n",
    "#This function needed because not all Brownian motion files start with 0, and they should for consistency\n",
    "def add_zero(path):\n",
    "    if path[0,] != 0:\n",
    "        path = np.insert(path, 0, 0)\n",
    "    return path\n",
    "    \n",
    "test1 = np.array([1, 2, 3])\n",
    "test2 = np.array([0, -1, -2, -3])\n",
    "assert_almost_equal(np.array([0, 1, 2, 3]), add_zero(test1))\n",
    "assert_almost_equal(test2, add_zero(test2))\n",
    "\n",
    "def get_diffs(path):\n",
    "    # Get the differences of a path i.e. the individual increments\n",
    "    return path[1:] - path[:-1]\n",
    "\n",
    "assert_almost_equal(get_diffs(np.array([1, 1, 1, 1, 1])), np.array([0, 0, 0, 0]))\n",
    "assert_almost_equal(get_diffs(np.array([1, 2, 3, 4, 5])), np.array([1, 1, 1, 1]))\n",
    "\n",
    "def second_order_diffs(path):\n",
    "    return path[2:] - 2 * path[1:-1] + path[:-2]\n",
    "\n",
    "assert_almost_equal(second_order_diffs(np.array([1, 1, 1, 1, 1])), np.array([0, 0, 0]))\n",
    "assert_almost_equal(second_order_diffs(np.array([1, 2, 3, 4, 5])), np.array([0, 0, 0]))\n",
    "\n",
    "#Get spot variances in spanned sets of a path\n",
    "def estimate_spot_variance(S_path, span):\n",
    "    #Implicitly assuming we are looking at a range[0, 1]\n",
    "    #Check can divide range into equal sub-sections\n",
    "    #Assume path starts with 0 at time 0\n",
    "\n",
    "    n_steps = S_path.shape[0] - 1\n",
    "    assert_almost_equal(n_steps % span, 0)\n",
    "    n_windows = n_steps / span\n",
    "    delta_t = 1 / n_windows\n",
    "\n",
    "    log_returns = np.log(S_path[1:]/S_path[:-1])\n",
    "    spanned_returns = np.split(log_returns, n_windows)\n",
    "\n",
    "    spot_variances = np.zeros(int(n_windows))\n",
    "\n",
    "    for i, v in enumerate(spanned_returns):\n",
    "        spot_variances[i] = np.dot(v, v) \n",
    "\n",
    "    return spot_variances\n",
    "\n",
    "test_path = np.arange(1, 22)\n",
    "assert_almost_equal(estimate_spot_variance(test_path, 10), np.array([0.8863009, 0.0432722]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Black-Scholes\n",
    "\n",
    "def get_d1_and_d2(S, t, K, T, r, sigma):\n",
    "    tau = T - t\n",
    "    d1 = 1/(sigma * np.sqrt(tau)) * (np.log(S/K) + (r + sigma ** 2 / 2) * tau)    \n",
    "    d2 = d1 - sigma * np.sqrt(tau)\n",
    "    return d1, d2\n",
    "\n",
    "def get_IV_from_price(S, t, K, T, r, market_price):\n",
    "\n",
    "    def black_scholes_call_price(sigma):\n",
    "        d1, d2 = get_d1_and_d2(S, t, K, T, r, sigma)\n",
    "        call_price = (S * norm.cdf(d1) - K * np.exp(-r * (T- t)) * norm.cdf(d2))\n",
    "        return call_price - market_price\n",
    "\n",
    "    return brentq(black_scholes_call_price, 1e-10, 100, maxiter = 500)\n",
    "\n",
    "assert_almost_equal(get_IV_from_price(100, 0, 100, 1, 0, 7.96558 ), 0.2, decimal = 5)\n",
    "assert_almost_equal(get_IV_from_price(100, 0, 90, 1, 0, 34.77223 ), 0.8, decimal = 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulation\n",
    "\n",
    "def simulate_bm_fbm(n_sims, n_steps, T, H, rng = None, do_both_paths = True):\n",
    "    #returns B and BH paths; both needed for simulation later\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(seed = 1729)\n",
    "    \n",
    "    variates = rng.normal(loc = 0, scale = 1, size = (n_sims, n_steps))\n",
    "    B_path = None\n",
    "    BH_path = np.zeros([n_sims, n_steps + 1])\n",
    "\n",
    "    delta = (T / n_steps)\n",
    "\n",
    "    if do_both_paths or H == 0.5:\n",
    "        B_path = np.zeros([n_sims, n_steps + 1])\n",
    "        for i in range(0, n_steps):\n",
    "            B_path[:, i + 1] = B_path[:, i] + delta ** 0.5 * variates[:, i]\n",
    "\n",
    "    if H != 0.5:\n",
    "        print('Beginning Simulation')\n",
    "        cov_mat = get_covariance_matrix(n_steps, H)\n",
    "        L = np.linalg.cholesky(cov_mat)\n",
    "        print('Covariance Done')\n",
    "        corr_variates = (L @ variates.T).T\n",
    "    \n",
    "        for i in range(0, n_steps):\n",
    "            BH_path[:, i + 1] = BH_path[:, i] + corr_variates[:, i] * delta ** H\n",
    "    else:\n",
    "        BH_path = B_path\n",
    "\n",
    "    return B_path, BH_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fbm_plots():\n",
    "\n",
    "    n_steps = 1024\n",
    "    T = 1\n",
    "\n",
    "    span = np.linspace(0, T, n_steps + 1)\n",
    "\n",
    "    out = simulate_bm_fbm(1, n_steps, T, 0.05)[1]\n",
    "    plt.plot(span, out.T, label = \"H = 0.05\")\n",
    "\n",
    "    out = simulate_bm_fbm(1, n_steps, T, 0.5)[1]\n",
    "    plt.plot(span, out.T, label = \"H = 0.5\")\n",
    "\n",
    "    out = simulate_bm_fbm(1, n_steps, T, 0.95)[1]\n",
    "    plt.plot(span, out.T, label = \"H = 0.90\")\n",
    "\n",
    "    plt.legend()\n",
    "make_fbm_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_estimator(x, T):\n",
    "    #return estimator of H from dissertation. Assume q = 2. \n",
    "\n",
    "    q = 2\n",
    "\n",
    "    if x[0,] != 0:\n",
    "        x = np.insert(x, 0, 0)\n",
    "    diffs = np.abs(x[1:] - x[:-1])\n",
    "    n_steps = diffs.shape[0]\n",
    "\n",
    "    return np.log(1/n_steps * np.sum(diffs ** q)) / (q * np.log(T / n_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simulation():\n",
    "\n",
    "    ### Test simulation\n",
    "\n",
    "    n_sims = 500\n",
    "    n_steps = 1024\n",
    "\n",
    "    H_vals = np.arange(0.1, 1, 0.1)\n",
    "\n",
    "    H_est = np.zeros(H_vals.shape)\n",
    "    H_sd = np.zeros(H_vals.shape)\n",
    "    tmp_H = np.zeros(n_sims)\n",
    "\n",
    "    for i, i_val in enumerate(H_vals):\n",
    "        sims = simulate_bm_fbm(n_sims, n_steps, 1, i_val)[1]\n",
    "        for j, j_sim in enumerate(sims):\n",
    "            tmp_H[j] = H_estimator(j_sim, 1)\n",
    "        H_est[i] = np.mean(tmp_H)\n",
    "        H_sd[i] = np.std(tmp_H)\n",
    "\n",
    "    return H_est, H_sd\n",
    "\n",
    "test_simulation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(H, x):\n",
    "    n_steps = x.shape[0]\n",
    "    R = get_covariance_matrix(n_steps, H)\n",
    "\n",
    "    determinant = np.linalg.slogdet(R)[1]\n",
    "    R_inv = np.linalg.inv(R)\n",
    "\n",
    "    return determinant  + n_steps * np.log(1 / n_steps * np.dot(x.T, np.dot(R_inv, x))) \n",
    "\n",
    "def max_MLE(x, T, scalar = None):\n",
    "\n",
    "    #Add x_0 value if it is missing\n",
    "    if x[0,] != 0:\n",
    "        x = np.insert(x, 0, 0)\n",
    "    \n",
    "    diffs = x[1:] - x[:-1]\n",
    "    n_steps = diffs.shape[0]\n",
    "    \n",
    "    #initial_guess = H_estimator(x, T)\n",
    "    initial_guess = 0.5\n",
    "\n",
    "    #Convergence issues noticed around H = 0.95, setting a bound of 0.94\n",
    "    #Set lower bound of floating epsilon to stop H hitting exactly 0, which will cause LLF errors\n",
    "    H = minimize(obj, initial_guess, args = (diffs), bounds = [(np.finfo(float).eps, 0.95)], method = \"Powell\", tol = 1e-10)\n",
    "\n",
    "    if H.success:\n",
    "        H_hat = H.x[0]\n",
    "        R = get_covariance_matrix(n_steps, H_hat)\n",
    "        if scalar is None:\n",
    "            scalar = (T / n_steps) ** (2 * H_hat)\n",
    "            R_inv = np.linalg.inv(R)\n",
    "            sigma_hat = np.sqrt(1 / (n_steps * scalar) * np.dot(diffs.T, np.dot(R_inv, diffs)))\n",
    "        else:\n",
    "            sigma_hat = scalar\n",
    "        return H_hat, sigma_hat\n",
    "\n",
    "    return H.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_MLE(scalar = 1):\n",
    "\n",
    "    ### Test simulation\n",
    "\n",
    "    n_sims = 50\n",
    "    n_steps = 100\n",
    "\n",
    "    H_vals = np.arange(0.25, 1, 0.25)\n",
    "\n",
    "    H_est = np.zeros(H_vals.shape)\n",
    "    H_sd = np.zeros(H_vals.shape)\n",
    "    scalar_est = np.zeros(H_vals.shape)\n",
    "    scalar_sd = np.zeros(H_vals.shape)\n",
    "    tmp_H = np.zeros(n_sims)\n",
    "    tmp_scalar = np.zeros(n_sims)\n",
    "\n",
    "    for i, i_val in enumerate(H_vals):\n",
    "        sims = simulate_bm_fbm(n_sims, n_steps, 1, i_val)[1] * scalar\n",
    "        for j, j_sim in enumerate(sims):\n",
    "            out = max_MLE(j_sim, 1)\n",
    "            tmp_H[j] = out[0]\n",
    "            tmp_scalar[j] = out[1]\n",
    "        H_est[i] = np.mean(tmp_H)\n",
    "        H_sd[i] = np.std(tmp_H)\n",
    "        scalar_est[i] = np.mean(tmp_scalar)\n",
    "        scalar_sd[i] = np.std(tmp_H)\n",
    "\n",
    "    return H_est, H_sd, scalar_est, scalar_sd\n",
    "\n",
    "test_MLE(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_path():\n",
    "\n",
    "    #sample_fbm = np.loadtxt('/Users/james.male/Dissertation/fBMPath2.txt')\n",
    "    sample_fbm = np.loadtxt('C:\\\\Users\\\\jamma\\Masters\\\\Dissertation\\\\fBMPath2.txt')\n",
    "    res = max_MLE(sample_fbm, 1)\n",
    "\n",
    "    comparison = res[1] * simulate_bm_fbm(1, 1024, 1, res[0])[1]\n",
    "\n",
    "    plt.plot(np.linspace(0, 1, 1024), sample_fbm, label = 'Original path')\n",
    "    plt.plot(np.linspace(0, 1, 1024), comparison[0, 1:], label = 'MLE simulated path')\n",
    "    plt.legend()\n",
    "\n",
    "    print (np.mean(comparison[0, 1:]), np.mean(sample_fbm))\n",
    "    print(np.std(comparison[0, 1:]),  np.std(sample_fbm))\n",
    "    print(np.min(comparison[0, 1:]),  np.min(sample_fbm))\n",
    "    print(np.max(comparison[0, 1:]),  np.max(sample_fbm))\n",
    "\n",
    "    return (res)\n",
    "\n",
    "test_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 With given Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFSV_model:\n",
    "    def __init__(self, V0, S0, V, rho, H, T, B_path, BH_path, W_path, window_length):\n",
    "        #inputs\n",
    "        self.V0 = V0\n",
    "        self.S0 = S0\n",
    "        self.V = V\n",
    "        self.rho = rho\n",
    "        self.rho_bar = np.sqrt(1 - rho ** 2)\n",
    "        self.window_length = window_length\n",
    "        self.H = H\n",
    "        self.T = T\n",
    "\n",
    "        #Derived\n",
    "        self.B_path = B_path\n",
    "        self.BH_path = BH_path\n",
    "        self.W_path = W_path\n",
    "        \n",
    "        self.n_steps = self.B_path.shape[0] - 1\n",
    "        self.delta = self.T * self.window_length / self.n_steps\n",
    "\n",
    "\n",
    "    def run_H_estimation(self):\n",
    "        self.simulate_RFSV()\n",
    "        self.variance_estimation()\n",
    "        return self.est_H, self.MLE_H, self.realised_H\n",
    "    \n",
    "    def simulate_RFSV(self):\n",
    "        \n",
    "        B_diffs = get_diffs(self.B_path)\n",
    "        W_diffs = get_diffs(self.W_path)\n",
    "\n",
    "        S_path = np.zeros(self.n_steps + 1)\n",
    "        V_path = np.zeros(self.n_steps + 1)\n",
    "\n",
    "        V_path = self.V0 * np.exp(self.V * self.BH_path)\n",
    "        S_path[0] = self.S0\n",
    "        \n",
    "        for i in range(self.n_steps):\n",
    "            S_path[i + 1] = S_path[i] *\\\n",
    "                (1 + np.sqrt(V_path[i]) *\\\n",
    "                (self.rho * B_diffs[i] + self.rho_bar * W_diffs[i]))\n",
    "        \n",
    "        self.V_path = V_path\n",
    "        self.S_path = S_path\n",
    "\n",
    "    def variance_estimation(self):\n",
    "        est_V = estimate_spot_variance(self.S_path, self.window_length)\n",
    "\n",
    "        V0_estimate = np.exp(np.mean(np.log(est_V)))\n",
    "        log_est_V = add_zero(np.log(est_V / V0_estimate))\n",
    "\n",
    "        #This gives us a variance process up to time 1 - delta\n",
    "        self.est_H = H_estimator(log_est_V, 1)\n",
    "\n",
    "        self.realised_H = H_estimator(add_zero(np.log(self.V_path / self.V0)), 1)\n",
    "\n",
    "        self.MLE_H = max_MLE(add_zero(est_V/self.V0), 1, scalar = self.V)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_path_loc_05 = 'C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\Bpath.05.txt'\n",
    "BH_path_loc_05 = 'C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\BHpath.05.txt'\n",
    "W_path_loc_05 = 'C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\Wpath.05.txt'\n",
    "\n",
    "# B_path_loc_05 = '/Users/james.male/Dissertation/Bpath.05.txt'\n",
    "# BH_path_loc_05 = '/Users/james.male/Dissertation/BHpath.05.txt'\n",
    "# W_path_loc_05 = '/Users/james.male/Dissertation/Wpath.05.txt'\n",
    "\n",
    "B_path_05 = add_zero(np.loadtxt(B_path_loc_05))\n",
    "BH_path_05 = add_zero(np.loadtxt(BH_path_loc_05))\n",
    "W_path_05 = add_zero(np.loadtxt(W_path_loc_05))\n",
    "\n",
    "window_lengths = [16, 32, 64]\n",
    "\n",
    "for window_length in window_lengths:\n",
    "    test = RFSV_model(0.1, 1, 1, -0.65, 0.05, 1, B_path_05, BH_path_05, W_path_05, window_length)\n",
    "\n",
    "    est_H, MLE_H, realised_H = test.run_H_estimation()\n",
    "    print(est_H, MLE_H, realised_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B_path_loc_05 = '/Users/james.male/Dissertation/Bpath.10.txt'\n",
    "# BH_path_loc_05 = '/Users/james.male/Dissertation/BHpath.10.txt'\n",
    "# W_path_loc_05 = '/Users/james.male/Dissertation/Wpath.10.txt'\n",
    "\n",
    "B_path_loc_10 = 'C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\Bpath.10.txt'\n",
    "BH_path_loc_10 = 'C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\BHpath.10.txt'\n",
    "W_path_loc_10 = 'C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\Wpath.10.txt'\n",
    "\n",
    "B_path_10 = add_zero(np.loadtxt(B_path_loc_10))\n",
    "BH_path_10 = add_zero(np.loadtxt(BH_path_loc_10))\n",
    "W_path_10 = add_zero(np.loadtxt(W_path_loc_10))\n",
    "\n",
    "window_lengths = [16, 32, 64]\n",
    "\n",
    "for window in window_lengths:\n",
    "    test = RFSV_model(0.1, 1, 1, -0.65, 0.1, 1, B_path_10, BH_path_10, W_path_10, window)\n",
    "\n",
    "    est_H, MLE_H, realised_H = test.run_H_estimation()\n",
    "    print(est_H, MLE_H, realised_H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 H = 0.5, 100 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_half_H_case():\n",
    "\n",
    "    n_sims = 100\n",
    "    n_steps = 2 ** 16\n",
    "\n",
    "    simulated_H = np.zeros(n_sims)\n",
    "    sim_MLE_H = np.zeros(n_sims)\n",
    "    sim_realised_H = np.zeros(n_sims)\n",
    "\n",
    "    S0 = 1\n",
    "    v = 1\n",
    "    rho = -0.65\n",
    "    V0 = 0.1\n",
    "    window_length = 512\n",
    "\n",
    "    rng = np.random.default_rng(seed = 1729)\n",
    "    B_paths = simulate_bm_fbm(n_sims, n_steps, 1, 0.5, rng)[0]\n",
    "    rng = np.random.default_rng(seed = 42)\n",
    "    W_paths = simulate_bm_fbm(n_sims, n_steps, 1, 0.5, rng)[0]\n",
    "\n",
    "    for i in range(n_sims):\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"{(i + 1) / 100:.0%}\")\n",
    "        model = RFSV_model(V0, S0, v, rho, 0.5, 1, B_paths[i, :], B_paths[i, :], W_paths[i, :], window_length)\n",
    "        est_H, MLE_H, realised_H = model.run_H_estimation()\n",
    "\n",
    "        simulated_H[i] = est_H\n",
    "        sim_MLE_H[i] = MLE_H\n",
    "        sim_realised_H[i] = realised_H\n",
    "\n",
    "    plt.hist(sim_MLE_H)\n",
    "\n",
    "    return sim_MLE_H, sim_realised_H\n",
    "\n",
    "\n",
    "sim_MLE_H, sim_realised_H = run_half_H_case()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim_MLE_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sim_realised_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(sim_MLE_H))\n",
    "print(np.std(sim_MLE_H))\n",
    "print(np.mean(sim_MLE_H) - 0.5)\n",
    "\n",
    "print(np.mean(sim_MLE_H) + np.std(sim_MLE_H)* norm.ppf(0.025))\n",
    "print(np.mean(sim_MLE_H) + np.std(sim_MLE_H)* norm.ppf(0.975))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(sim_realised_H))\n",
    "print(np.std(sim_realised_H))\n",
    "print(np.mean(sim_realised_H) - 0.5)\n",
    "\n",
    "print(np.mean(sim_realised_H) + np.std(sim_realised_H)* norm.ppf(0.025))\n",
    "print(np.mean(sim_realised_H) + np.std(sim_realised_H)* norm.ppf(0.975))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Monte Carlo Pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_Pricing_RFSV():\n",
    "\n",
    "    n_sims = 10000\n",
    "    n_steps = 2 ** 14 #consistent with simulated sample paths\n",
    "\n",
    "    S0 = 1\n",
    "    v = 1\n",
    "    rho = 0.65\n",
    "    V0 = 0.04\n",
    "    H = 0.1\n",
    "    dummy_window_length = 512\n",
    "\n",
    "    log_moneyness = np.arange(-0.5, 0.55, 0.05)\n",
    "    call_prices = np.zeros(log_moneyness.shape)\n",
    "    IVs = np.zeros(log_moneyness.shape)\n",
    "\n",
    "    rng = np.random.default_rng(seed = 1729)\n",
    "    B_sim, BH_sim = simulate_bm_fbm(int(n_sims / 2), n_steps, 1, H, rng)\n",
    "    rng = np.random.default_rng(seed = 42)\n",
    "    W_sim = simulate_bm_fbm(int(n_sims / 2), n_steps, 1, 0.5, rng)[0]\n",
    "    print('Simulation done')\n",
    "\n",
    "    B_paths = np.vstack((B_sim, -B_sim))\n",
    "    BH_paths = np.vstack((BH_sim, -BH_sim))\n",
    "    W_paths = np.vstack((W_sim, -W_sim))\n",
    "    S1_paths = np.zeros([n_sims, 1])\n",
    "\n",
    "    for i in range  (n_sims):\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"{(i + 1) / n_sims:.0%}\")\n",
    "        model = RFSV_model(V0, v, S0, rho, 0.1, 1, B_paths[i, :], B_paths[i, :], W_paths[i, :], dummy_window_length)\n",
    "        model.simulate_RFSV()\n",
    "        S1_paths[i] = model.S_path[-1]\n",
    "\n",
    "    for i, v in enumerate(log_moneyness):\n",
    "        strike = S0 * np.exp(v)\n",
    "        call_prices[i] = np.average(np.maximum(S1_paths - strike, 0))\n",
    "        IVs[i] = get_IV_from_price(S0, 0, strike, 1, 0, call_prices[i])\n",
    "\n",
    "    plt.plot(log_moneyness, IVs)\n",
    "    return IVs\n",
    "\n",
    "MC_Pricing_RFSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(log_moneyness):\n",
    "    strike = S0 * np.exp(v)\n",
    "    call_prices[i] = np.average(np.maximum(S1_paths - strike, 0))\n",
    "    IVs[i] = get_IV_from_price(S0, 0, strike, 1, 0, call_prices[i])\n",
    "\n",
    "IVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(log_moneyness, IVs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Further Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPX_path = \"C:\\\\Users\\\\jamma\\\\Masters\\\\Dissertation\\\\SPX3yrDailyRealizedVariance1minBins.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPX_path = np.loadtxt(SPX_path)\n",
    "df_describe = pd.DataFrame(SPX_path)\n",
    "df_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(SPX_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_SPX_graphs(SPX_data, V0_input = None, make_graphs = True):\n",
    "\n",
    "    if V0_input is None:\n",
    "        V0_input = np.exp(np.mean(np.log(SPX_data)))\n",
    "\n",
    "    SPX_adjusted = add_zero(np.log(SPX_data / V0_input ))\n",
    "    \n",
    "    H_hat_SPX, sigma_hat_SPX = max_MLE(SPX_adjusted, SPX_adjusted.shape[0])\n",
    "    span = np.linspace(0, SPX_adjusted.shape[0], SPX_adjusted.shape[0])\n",
    "    out = simulate_bm_fbm(1, SPX_adjusted.shape[0] - 1, SPX_adjusted.shape[0] - 1, H_hat_SPX)[1]\n",
    "    \n",
    "    if make_graphs:\n",
    "        plt.figure(0)\n",
    "        plt.plot(SPX_adjusted)\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(1)\n",
    "        plt.subplot(211)\n",
    "        plt.plot(span, SPX_adjusted)\n",
    "        plt.subplot(212)\n",
    "        plt.plot(span, out.T * sigma_hat_SPX)\n",
    "        plt.show()\n",
    "\n",
    "    return H_hat_SPX, sigma_hat_SPX, V0_input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approach 1 - assume V0 is as given in model\n",
    "\n",
    "V0_given = 0.1\n",
    "\n",
    "make_SPX_graphs(SPX_path, V0_given)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_SPX_graphs(SPX_path[-251:], V0_given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_H = np.zeros(40)\n",
    "span = np.zeros(40)\n",
    "\n",
    "for i in range(40):\n",
    "    rolling_H[i] = make_SPX_graphs(SPX_path[19 * i:], V0_given, make_graphs = False)[0]\n",
    "    span[i] = len(SPX_path) - 19 * i\n",
    "\n",
    "plt.plot(span, rolling_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(20), rolling_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Own investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class estimator_master():\n",
    "    def __init__(self,\n",
    "        n_sims = 5_000,\n",
    "        n_steps = 2 ** 14,\n",
    "        H_values = np.array([0.5]),\n",
    "        do_convergence_tests = True,\n",
    "        T = 1\n",
    "        ):\n",
    "\n",
    "        self.H_values = H_values\n",
    "        self.H_simulated = np.zeros((n_sims, H_values.shape[0]))\n",
    "\n",
    "        self.norm_checks_JB = np.zeros(H_values.shape)\n",
    "        self.norm_checks_LF = np.zeros(H_values.shape)\n",
    "\n",
    "        # Distribution Check\n",
    "        self.n_sims = n_sims\n",
    "        self.n_steps = n_steps\n",
    "        self.T = T\n",
    "\n",
    "        self.do_convergence_tests = do_convergence_tests\n",
    "\n",
    "    def do_analysis(self):\n",
    "        self.run_simulation()\n",
    "        if self.do_convergence_tests:\n",
    "            self.convergence_testing()\n",
    "        self.do_analytics()\n",
    "\n",
    "    def H_estimator(self):\n",
    "        pass\n",
    "\n",
    "    def run_simulation(self):\n",
    "\n",
    "        fig, axs = plt.subplots(int(np.ceil(self.H_values.shape[0] / 4)), 4, figsize=(40, 20))\n",
    "        \n",
    "        \n",
    "        for j, H in enumerate(self.H_values):\n",
    "            print(H)\n",
    "            out = simulate_bm_fbm(self.n_sims, self.n_steps, self.T, H, do_both_paths = False)[1]\n",
    "            \n",
    "            for i in range(out.shape[0]):\n",
    "                if i % 1_000 == 0:\n",
    "                    print(i)\n",
    "                self.H_simulated[i, j] = self.H_estimator(out[i, :])\n",
    "\n",
    "            self.norm_checks_JB[j] = jarque_bera(self.H_simulated[:, j]).pvalue\n",
    "            self.norm_checks_LF[j] = lilliefors(self.H_simulated[:, j])[1]\n",
    "        \n",
    "            axs[int(np.floor(j / 4)), j % 4].hist(self.H_simulated[:, j], bins = 40, facecolor='blue', alpha=0.5)\n",
    "            \n",
    "            y = np.linspace(self.H_simulated[:, j].min(), self.H_simulated[:, j].max(), 1000)\n",
    "            bin_width = (self.H_simulated[:, j].max() - self.H_simulated[:, j].min()) / 40\n",
    "            axs[int(np.floor(j / 4)), j % 4].plot(y, norm.pdf(y, loc = np.mean(self.H_simulated[:, j]), scale = np.std(self.H_simulated[:, j])) * self.n_sims * bin_width)\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "        \n",
    "    def convergence_testing(self):\n",
    "\n",
    "        exponents = np.arange(3, 13, 1)\n",
    "        self.H_by_density = np.zeros((len(self.H_values), len(exponents)))\n",
    "        n_sims = 5_000\n",
    "\n",
    "        fig, axs = plt.subplots(int(np.ceil(self.H_values.shape[0] / 4)), 4, figsize=(40, 20))\n",
    "\n",
    "        for i, H in enumerate(self.H_values):\n",
    "            print(H)\n",
    "            out = simulate_bm_fbm(n_sims, int(2 ** (exponents[-1])), 1, self.H_values[i], do_both_paths = False)[1]\n",
    "\n",
    "            for j in range(len(exponents)):\n",
    "                width = 2 ** (exponents[-1] - exponents[j])\n",
    "                tmp_out = out[:, 0::width]\n",
    "                tmp_estimators = np.zeros(n_sims)\n",
    "            \n",
    "                for k in range(out.shape[0]):\n",
    "                    tmp_estimators[k] = self.H_estimator(tmp_out[k, :])\n",
    "                \n",
    "                self.H_by_density[i, j] = np.mean(tmp_estimators)\n",
    "            \n",
    "            axs[int(np.floor(i / 4)), i % 4].plot(exponents, np.log2(self.H_by_density[i, :]))\n",
    "            axs[int(np.floor(i / 4)), i % 4].plot(exponents, np.tile(np.log2(H), len(exponents)))\n",
    "            \n",
    "        fig.show()\n",
    "        \n",
    "    def do_analytics(self):\n",
    "        mean_H = np.zeros(self.H_values.shape)\n",
    "        sd_H = np.zeros(self.H_values.shape)\n",
    "        for j, H in enumerate(self.H_values):\n",
    "            mean_H[j] = np.mean(self.H_simulated[:, j])\n",
    "            sd_H[j] = np.std(self.H_simulated[:, j])\n",
    "        self.df_analytics = pd.DataFrame(np.column_stack((mean_H, sd_H, self.norm_checks_JB)), index = self.H_values, columns = [\"Mean\", \"Standard Deviation\", \"JB p-value\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Gladyshev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gladyshev_estimator(estimator_master):\n",
    "    def H_estimator(self, x):\n",
    "\n",
    "        #return estimator of H from dissertation. Assume q = 2. \n",
    "        q = 2\n",
    "\n",
    "        if x[0,] != 0:\n",
    "            x = np.insert(x, 0, 0)\n",
    "        diffs = get_diffs(x)\n",
    "        n_steps = diffs.shape[0]\n",
    "\n",
    "        return np.log(1/n_steps * np.sum(diffs ** q)) / (q * np.log(self.T / n_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "glady = gladyshev_estimator(\n",
    "    H_values = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    n_sims = 5_000,\n",
    "    n_steps = 2 ** 14,\n",
    ")\n",
    "\n",
    "glady.do_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glady.df_analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glady.norm_checks_JB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Istas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class istas(estimator_master):\n",
    "    def H_estimator(self, x):\n",
    "\n",
    "        n_steps = x.shape[0]\n",
    "        x = add_zero(x)\n",
    "\n",
    "        full_path_second_order = np.sum(second_order_diffs(x) ** 2)\n",
    "        half_path_second_order = np.sum(second_order_diffs(x[::2]) ** 2)\n",
    "\n",
    "\n",
    "        return 0.5 - 1 / ( 2 * np.log(2)) * np.log(full_path_second_order / half_path_second_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ist1 = istas(\n",
    "    H_values = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    n_sims = 5_000,\n",
    "    n_steps = 2 ** 14,\n",
    ")\n",
    "\n",
    "ist1.do_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ist1.df_analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Coeurjolly 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class coeurjolly2(estimator_master):\n",
    "    def H_estimator(self, x):\n",
    "\n",
    "        n_steps = x.shape[0]\n",
    "        def coeur_function(y):\n",
    "            return (self.T / n_steps) ** (2 * y) * (4 - 2 ** (2 * y))\n",
    "\n",
    "        x = add_zero(x)\n",
    "        self.path_diffs = 1 / n_steps * np.sum(second_order_diffs(x) ** 2)\n",
    "\n",
    "        def obj_coeur(z):\n",
    "            return coeur_function(z) - self.path_diffs\n",
    "        \n",
    "        return brentq(obj_coeur, -1, 1, maxiter = 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeur2 = coeurjolly2(\n",
    "    H_values = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
    "    n_sims = 5_000,\n",
    "    n_steps = 2 ** 14,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeur2.do_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeur2.norm_checks_LF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeur2.df_analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Exact CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions\n",
    "\n",
    "### Source: J.-C. Breton et al./Exact intervals for the Hurst parameter \n",
    "\n",
    "\n",
    "def g_inv(n_steps, target):  \n",
    "\n",
    "    def g(x):\n",
    "        return x - (np.log(4 - 4 ** x))/(2 * np.log(n_steps))\n",
    "\n",
    "    def g_obj(x):\n",
    "        return g(x) - target  \n",
    "    \n",
    "    return brentq(g_obj, 0, 1, maxiter = 500)\n",
    "\n",
    "def exp_fn(a, n):\n",
    "    return np.exp(-a ** 2 / (71 * (a/np.sqrt(n) + 3)))\n",
    "\n",
    "def prob_function(a, n):\n",
    "    return max(1 - 2 * exp_fn(a, n), 0)\n",
    "\n",
    "def derive_CI(fbm_path, error_range, a):\n",
    "    H_star = 0.5\n",
    "\n",
    "    n_steps = fbm_path.shape[0] - 1\n",
    "\n",
    "    assert(exp_fn(a, n_steps)) <= error_range / 2\n",
    "    assert(a < (4 - 4 ** H_star) * np.sqrt(n_steps))\n",
    "\n",
    "    quad_variations = np.sum(second_order_diffs(fbm_path) ** 2)\n",
    "\n",
    "    lower = 0.5 \\\n",
    "    - np.log(quad_variations) / (2 * np.log(n_steps)) \\\n",
    "    + np.log(1 - a / ((4 - 4 ** H_star) * np.sqrt(n_steps)))/(2 * np.log(n_steps))\n",
    "    upper = 0.5 \\\n",
    "    - np.log(quad_variations) / (2 * np.log(n_steps)) \\\n",
    "    + np.log(1 + a / ((4 - 4 ** H_star) * np.sqrt(n_steps)))/(2*np.log(n_steps))\n",
    "\n",
    "    lower = max(lower, - np.log(3)/(2 * np.log(n_steps))) \n",
    "\n",
    "    return g_inv(n_steps, lower), g_inv(n_steps, upper), prob_function(a, n_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Simulation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Done\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 12\u001b[0m     lower, upper, prob \u001b[38;5;241m=\u001b[39m \u001b[43mderive_CI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(lower, upper)\n\u001b[0;32m     16\u001b[0m     lower_CI[i] \u001b[38;5;241m=\u001b[39m lower\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mderive_CI\u001b[1;34m(fbm_path, error_range, a)\u001b[0m\n\u001b[0;32m     30\u001b[0m quad_variations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(second_order_diffs(fbm_path) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     32\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \\\n\u001b[0;32m     33\u001b[0m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(quad_variations) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps)) \\\n\u001b[0;32m     34\u001b[0m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m/\u001b[39m ((\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m H_star) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(n_steps)))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps))\n\u001b[1;32m---> 35\u001b[0m upper \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \\\n\u001b[0;32m     36\u001b[0m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(quad_variations) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps)) \\\n\u001b[0;32m     37\u001b[0m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m a \u001b[38;5;241m/\u001b[39m ((\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m H_star) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(n_steps)))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(n_steps))\n\u001b[0;32m     39\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lower, \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps))) \n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_inv(n_steps, lower), g_inv(n_steps, upper), prob_function(a, n_steps)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mderive_CI\u001b[1;34m(fbm_path, error_range, a)\u001b[0m\n\u001b[0;32m     30\u001b[0m quad_variations \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(second_order_diffs(fbm_path) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     32\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \\\n\u001b[0;32m     33\u001b[0m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(quad_variations) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps)) \\\n\u001b[0;32m     34\u001b[0m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m a \u001b[38;5;241m/\u001b[39m ((\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m H_star) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(n_steps)))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps))\n\u001b[1;32m---> 35\u001b[0m upper \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \\\n\u001b[0;32m     36\u001b[0m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(quad_variations) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps)) \\\n\u001b[0;32m     37\u001b[0m \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m a \u001b[38;5;241m/\u001b[39m ((\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m H_star) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(n_steps)))\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mlog(n_steps))\n\u001b[0;32m     39\u001b[0m lower \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(lower, \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(n_steps))) \n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m g_inv(n_steps, lower), g_inv(n_steps, upper), prob_function(a, n_steps)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1366\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1291\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1253\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jamma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2023\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2020\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2022\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 2023\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2025\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2028\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jamma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2059\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2059\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "H = 0.25\n",
    "a = 26\n",
    "n = 1700\n",
    "\n",
    "paths = simulate_bm_fbm(10, n, n, H)[1]\n",
    "lower_CI = np.zeros(100)\n",
    "upper_CI = np.zeros(100)\n",
    "in_range = np.ones(100)\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    lower, upper, prob = derive_CI(paths[i], 0.2, a) \n",
    "    \n",
    "    print(lower, upper)\n",
    "\n",
    "    lower_CI[i] = lower\n",
    "    upper_CI[i] = upper\n",
    "\n",
    "    if lower_CI[i] > H or upper_CI[i] < H:\n",
    "        in_range[i] = 0\n",
    "\n",
    "print(sum(in_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.04315810945307116, 0.9554507277082929)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derive_CI(sample_fbm, 0.05, 33) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_order_diffs(path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Random_Stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
